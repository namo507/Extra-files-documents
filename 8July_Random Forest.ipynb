{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification\n",
    "1) Random Forest is an ensemble machine learning technique capable of performing both regression and classification tasks using multiple decision trees and a statistical technique called Bagging (Bootstrap Aggregation).<br>\n",
    "2) A RF besides just averaging the prediction of trees it uses two key concepts that give it the name random:<br>\n",
    "\n",
    "<b>\n",
    "a) Random sampling of training observations (rows) when building trees.<br>\n",
    "b) Random subsets of features (columns) for splitting nodes.<br>\n",
    "</b>\n",
    "\n",
    "3) Random forest builds multiple decision trees and merge their predictions together to get a more accurate and stable prediction rather than relying on individual decision trees.<br>\n",
    "4) It combines the simplicity of the Decision Tree with flexibility resulting in a vast increase in accuracy.<br>\n",
    "5) It combines a number of weak estimators (individual decision trees) and merges them into a strong estimator.<br>\n",
    "6) Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction.<br>\n",
    "7) <b>Random Forest Intuition </b>- A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.<br>\n",
    "8) It uses Bootstrapping (selection with replacement) to select random subsets of records and/or columns for creating individual decision trees. It means that some samples might be used multiple times in a single tree.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "1) There are 100 rows of data<br>\n",
    "2) The target variable(y) has 2 categories - A and B<br>\n",
    "3) In Random Forest, we are constructing 7 Decision Trees<br>\n",
    "4) Suppose out of 7 DT (DT1,DT2,......,DT7), row 37 is a part of DT1, DT3, DT4, DT5, DT6.<br>\n",
    "5) Predictions generated by Decision Trees for row no 37 are:<br>\n",
    "        \n",
    "       DT1 -> A\n",
    "       DT3 -> B\n",
    "       DT4 -> A\n",
    "       DT5 -> A\n",
    "       DT6 -> B\n",
    "6) Prediction out of Random Forest for row no 37 is mode of these predictions (3A and 2B) -> mode(3,2) = 3<br>\n",
    "7) Prediction for Row no 37 is Category A according to Random Forest.\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping\n",
    "Bootstrapping is randomly selecting samples from training data with replacement. The samples so generated are called as bootstrap samples.\n",
    "\n",
    "<img src=\"rf1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working of Random Forest\n",
    "\n",
    "1)\tSelect random samples from a given dataset.<br>\n",
    "2)\tConstruct a decision tree for each sample and get a prediction result from each decision tree.<br>\n",
    "3)\tPerform a vote for each predicted result.<br>\n",
    "4)\tSelect the prediction result with the most votes as the final prediction.<br>\n",
    "\n",
    "a)\tThe random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. <br>\n",
    "b)\tThe final predictions of the random forest are made by averaging the predictions of each individual tree. <br>\n",
    "c)\tThe random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree. <br>\n",
    "\n",
    "Example<br>\n",
    "Target variable has 3 catgories => A, B and C<br>\n",
    "50 decision trees are made as a part of Random Forest<br>\n",
    "33 DT generate category A as its prediction, 10 DT generate category B as its prediction\n",
    "7 DT generate category C as its prediction.\n",
    "\n",
    "Avergaging or Max Frequency or Voting => max(33,10,7) = 33,<br>\n",
    "Conclusion - DT generated category A<br>\n",
    "=> So the prediction of Random Forest is category A<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of RandomForest\n",
    "\n",
    "<b>1) n_estimators :</b>  This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower.\n",
    "\n",
    "<b>2) min_samples_split :</b> The minimum number of samples required to split an internal node. The algorithm keeps splitting the nodes as long as a node has more samples (data points) than the number specified with min_samples_split parameter.\n",
    "\n",
    "<b>3) criterion:</b> gini or entropy\n",
    "\n",
    "<b>4) max_features:</b> These are the maximum number of features Random Forest is allowed to try in individual tree. The values it can have are:-<br>\n",
    "a) Auto/None : This will simply take all the features which make sense in every tree. Here we simply do not put any restrictions on the individual tree.<br>\n",
    "b) sqrt : This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree. “log2” is another similar type of option for max_features.<br>\n",
    "c) 0.x : This option allows the random forest to take x% of variables in individual run. We can assign and value in a format “0.x” where we want x% of features to be considered.\n",
    "\n",
    "<b>5) bootstrap :</b>  true/false. If it is set to true samples are drawn with replacement and samples are drawn without replacement if bootstrap is set to false\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros \n",
    "\n",
    "1) Random Forest is considered as a highly accurate and robust method because of the number of decision trees participating in the process.<br>\n",
    "2) It overcomes the problem of overfitting faced by decision tree. The main reason is that it takes the average of all the predictions.<br>\n",
    "\n",
    "#### Cons\n",
    "\n",
    "1) Random forest is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
