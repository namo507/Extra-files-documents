{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "1) Decision-tree algorithm falls under the category of supervised learning algorithms. <br>\n",
    "2) Decision Tress is used for both classification and regression. <br>\n",
    "3) <b>A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).</b><br>\n",
    "4) The decision tree from the name itself signifies that it is used for making decisions from the given dataset. The concept behind the decision tree is that it helps to select appropriate features for splitting the tree into subparts.<br>\n",
    "5) It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes.<br>\n",
    "6) Decision tree uses the tree representation to solve the problem in which each leaf node corresponds to a class label(target variable) and attributes are represented on the internal node of the tree.<br>\n",
    "\n",
    "Example<br>\n",
    "We have 100 rows of data with<br>\n",
    "x = 'Age', 'Gender', 'BMI','Body_wt', 'Blood_Glucose_Level'<br>\n",
    "y = 0 or 1 [0 = Non-diabteic, 1 = Diabetic]<br>\n",
    "65 are diabetic and 35 are non-diabetic\n",
    "\n",
    "<img src=\"dt1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DT_example.png\" height=\"300\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm\n",
    "1) We use statistical methods <b>(gini or entropy)</b> for ordering attributes as root or the internal node.\n",
    "\n",
    "### 1) Entropy\n",
    "Sample Reference -> https://www.saedsayad.com/decision_tree.htm\n",
    "\n",
    "1)\tEntropy is the measure of uncertainty or disorder or impurity .It characterizes the impurity of an arbitrary collection of examples. The higher the entropy more the information content.<br>\n",
    "2)\tIf the sample is completely homogeneous the entropy is zero and if the sample is equally divided it has entropy of one.\n",
    "<img src=\"dt2.png\">\n",
    "where<br>\n",
    "pi = probability of label i<br>\n",
    "n = number of categories in the label <br>\n",
    "\n",
    "<b>Information Gain</b><br>\n",
    "a)\tIt is based on decrease in entropy after dataset is split on an attribute/(feature or column)<br>\n",
    "b)\tThe split with highest information gain is chosen as an internal node.<br>\n",
    "\n",
    "<b>\n",
    "E(Target_variable) = Σ-plog(p)  <br>\n",
    "E(Target_variable,feature) = P(feature)*E(Target) <br>\n",
    "Information Gain = Entropy(Target_variable) - Entropy(Target_variable,feature), \n",
    "</b>\n",
    "    \n",
    "### 2) Gini\n",
    "1)\tGini Index is a metric to measure how often a randomly chosen element would be incorrectly identified.<br>\n",
    "2)\tIt means an attribute with lower Gini index should be preferred.<br>\n",
    "3)\t<b>Gini = 1 – Σ (Pi)^2 for i=1 to number of classes</b><br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros\n",
    "\n",
    "1) Works well with numerical & categorical features.<br>\n",
    "2) Decision Tree can handle non-linear data.<br>\n",
    "3) Performs well on large datasets.<br>\n",
    "\n",
    "#### Cons\n",
    "\n",
    "1) It suffers from overfitting. Random Forest mitigates this issue.<br>\n",
    "2) Training  time is high.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "1) criterion - entropy or gini\n",
    "\n",
    "2) max_depth - max depth of decision tree\n",
    "\n",
    "3) min_samples_split - Min number of samples beyond which an internal node gets coverted to a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Decision Trees_Ex1-MWF-10.-11.30am.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
