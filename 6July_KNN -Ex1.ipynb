{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0186ad",
   "metadata": {},
   "source": [
    "### KNN Algorithm (K-Nearest Neighbors Algorithm)\n",
    "\n",
    "1) It is a supervised learning algorithm used for Classification.<br>\n",
    "\n",
    "2) KNN Intuition - The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.<br>\n",
    "\n",
    "3) KNN works by finding the distances between a query(test data) and all the examples in the data(train data), selecting the specified number of examples (K) closest to the query, then votes for the most frequent label (in the case of classification).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a03ade2",
   "metadata": {},
   "source": [
    "### KNN Classification Algorithm\n",
    "\n",
    "1) Load the data<br>\n",
    "2) Initialize K to your chosen number of neighbors.<br>\n",
    "3) For getting the predicted class, iterate from 1 to total number of training data points<br> \n",
    "\n",
    "a) Calculate the distance between test data and each row of training data. Use Euclidean distance as our distance metric since it’s the most popular method. The other metrics that can be used are Manhattan, etc.<br>\n",
    "b) Sort the calculated distances in ascending order based on distance values<br>\n",
    "c) Get top K rows from the sorted array<br>\n",
    "d) Get the most frequent class(mode) of these rows<br>\n",
    "e) Return the predicted class<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2bd69f",
   "metadata": {},
   "source": [
    "#### Different Distance Metrics in K-NN\n",
    "\n",
    "1) Euclidean Distance((x1,y1),(x2,y2)) =  √((y2-y1)^2 + (x2-x1)^2) <br>\n",
    "2) Manhattan Distance((x1,y1),(x2,y2)) = |(x2-x1)|  + |(y2-y1)| <br>\n",
    "3) Minkowski’s Distance ((x1,y1),(x2,y2)) = (|x2-x1|^p +|y2-y1|^p)^(1/p)<br>\n",
    "Note<br>\n",
    "a) For p=2, Minkowski’s Distance equals Euclidean’s Distance <br>\n",
    "b) p=1, Minkowski’s Distance equals Manhattan Distance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a8c859",
   "metadata": {},
   "source": [
    "<img src=\"knn1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1693c1a",
   "metadata": {},
   "source": [
    "### Notes\n",
    "1) As we decrease the value of K to 1, our predictions become less stable. Just think for a minute, imagine K=1 and we have a query point surrounded by several reds and one green, but the green is the single nearest neighbor. Reasonably, we would think the query point is most likely red, but because K=1, KNN incorrectly predicts that the query point is green.\n",
    "\n",
    "2) Inversely, as we increase the value of K, our predictions become more stable due to majority voting / averaging, and thus, model is more likely to make more accurate predictions (up to a certain point). Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n",
    "\n",
    "3) In cases where we are taking a majority vote (e.g. picking the mode in a classification problem) among labels, we usually make K an odd number to have a tiebreaker.\n",
    "\n",
    "4) KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ba21bf",
   "metadata": {},
   "source": [
    "### Pros and Cons\n",
    "\n",
    "<b>Pros</b><br>\n",
    "1) Low computation time. <br>\n",
    "2) Simple and easy to implement.<br>\n",
    "\n",
    "<b>Cons</b><br>\n",
    "1) It assumes that similar things occur in close proximity which can’t be generalized for every single scenario. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caca565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201a5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
